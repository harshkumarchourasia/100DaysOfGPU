{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0bb3458-ba71-4acd-9700-ef119a2cf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "1.9010239839553833\n",
      "1.9058560132980347\n",
      "1.9211519956588745\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(torch.square(a))\n",
    "print(a ** 2)\n",
    "print(a * a)\n",
    "\n",
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)\n",
    "\n",
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "def square_3(a):\n",
    "    return a ** 2\n",
    "\n",
    "print(time_pytorch_function(torch.square, b))\n",
    "print(time_pytorch_function(square_2, b))\n",
    "print(time_pytorch_function(square_3, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72acc7d3-9862-4f9c-b75b-81a2772c3387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "             aten::square         2.49%      45.536us        11.79%     216.030us     216.030us      48.000us         2.28%       2.106ms       2.106ms             1  \n",
      "                aten::pow         5.66%     103.755us         8.67%     158.767us     158.767us       2.036ms        96.68%       2.058ms       2.058ms             1  \n",
      "        aten::result_type         0.30%       5.431us         0.30%       5.431us       5.431us      14.000us         0.66%      14.000us      14.000us             1  \n",
      "                 aten::to         0.07%       1.196us         0.07%       1.196us       1.196us       8.000us         0.38%       8.000us       8.000us             1  \n",
      "          cudaEventRecord         2.67%      48.821us         2.67%      48.821us       6.103us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "         cudaLaunchKernel         1.85%      33.811us         1.85%      33.811us      33.811us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        86.98%       1.593ms        86.98%       1.593ms       1.593ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.832ms\n",
      "Self CUDA time total: 2.106ms\n",
      "\n",
      "=============\n",
      "Profiling a * a\n",
      "=============\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::mul         3.55%      68.783us         5.12%      98.999us      98.999us       1.998ms       100.00%       1.998ms       1.998ms             1  \n",
      "          cudaEventRecord         1.22%      23.573us         1.22%      23.573us      11.787us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "         cudaLaunchKernel         1.56%      30.216us         1.56%      30.216us      30.216us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        93.67%       1.813ms        93.67%       1.813ms       1.813ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.935ms\n",
      "Self CUDA time total: 1.998ms\n",
      "\n",
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::pow         5.22%     103.636us         7.87%     156.272us     156.272us       2.031ms        98.98%       2.052ms       2.052ms             1  \n",
      "        aten::result_type         0.15%       2.916us         0.15%       2.916us       2.916us      11.000us         0.54%      11.000us      11.000us             1  \n",
      "                 aten::to         0.08%       1.504us         0.08%       1.504us       1.504us      10.000us         0.49%      10.000us      10.000us             1  \n",
      "          cudaEventRecord         1.95%      38.715us         1.95%      38.715us       6.453us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "         cudaLaunchKernel         1.64%      32.505us         1.64%      32.505us      32.505us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        90.97%       1.807ms        90.97%       1.807ms       1.807ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.986ms\n",
      "Self CUDA time total: 2.052ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_462/3816358504.py:6: FutureWarning: The attribute `use_cuda` will be deprecated soon, please use ``use_device = 'cuda'`` instead.\n",
      "  with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
      "/tmp/ipykernel_462/3816358504.py:15: FutureWarning: The attribute `use_cuda` will be deprecated soon, please use ``use_device = 'cuda'`` instead.\n",
      "  with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
      "/tmp/ipykernel_462/3816358504.py:24: FutureWarning: The attribute `use_cuda` will be deprecated soon, please use ``use_device = 'cuda'`` instead.\n",
      "  with torch.autograd.profiler.profile(use_cuda=True) as prof:\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")\n",
    "\n",
    "# Now profile each function using pytorch profiler\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7fe9d-8427-400c-910e-c7280c2feeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd32eb-cf31-4e00-b9d4-8d03f5faab55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5278b4-5e89-4225-b185-f191b89a3b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us     178.877ms        56.71%     178.877ms      89.438ms             2  \n",
      "                                            aten::copy_         0.00%      94.101us         6.30%     133.200ms      66.600ms     132.709ms        42.08%     132.709ms      66.355ms             2  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     132.709ms        42.08%     132.709ms      66.355ms             2  \n",
      "                                              aten::pow         0.01%     122.323us         0.01%     200.549us     100.274us       3.810ms         1.21%       3.810ms       1.905ms             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.810ms         1.21%       3.810ms       1.905ms             2  \n",
      "                                          ProfilerStep*         2.01%      42.554ms        99.92%        2.113s        1.057s       0.000us         0.00%     136.519ms      68.260ms             2  \n",
      "                                            aten::randn         0.00%      60.828us        91.59%        1.937s     968.470ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                            aten::empty         0.00%      55.458us         0.00%      55.458us      27.729us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                          aten::normal_        91.58%        1.937s        91.58%        1.937s     968.412ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                               aten::to         0.00%      67.361us         6.31%     133.409ms      33.352ms       0.000us         0.00%     132.709ms      33.177ms             4  \n",
      "                                         aten::_to_copy         0.00%      67.176us         6.31%     133.341ms      66.671ms       0.000us         0.00%     132.709ms      66.355ms             2  \n",
      "                                    aten::empty_strided         0.00%      74.499us         0.00%      74.499us      37.250us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        cudaMemcpyAsync         6.28%     132.733ms         6.28%     132.733ms      66.367ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaStreamSynchronize         0.02%     372.453us         0.02%     372.453us     186.227us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                           aten::square         0.00%      13.214us         0.01%     213.763us     106.881us       0.000us         0.00%       3.810ms       1.905ms             2  \n",
      "                                      aten::result_type         0.00%       3.147us         0.00%       3.147us       1.573us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         0.00%      73.877us         0.00%      73.877us      36.939us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaDeviceSynchronize         0.08%       1.705ms         0.08%       1.705ms       1.705ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.115s\n",
      "Self CUDA time total: 315.396ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "# ## Default way to use profiler\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "#     for _ in range(10):\n",
    "#         a = torch.square(torch.randn(10000, 10000).cuda())\n",
    "\n",
    "# prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "\n",
    "## With warmup and skip\n",
    "# https://pytorch.org/docs/stable/profiler.html\n",
    "\n",
    "# Non-default profiler schedule allows user to turn profiler on and off\n",
    "# on different iterations of the training loop;\n",
    "# trace_handler is called every time a new trace becomes available\n",
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),\n",
    "    on_trace_ready=trace_handler\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            torch.square(torch.randn(10000, 10000).cuda())\n",
    "            # send a signal to the profiler that the next iteration has started\n",
    "            p.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169ef6e-98bb-4e03-9518-cb91ae32b057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe158eb-761b-4264-8d09-a60a61de15c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837749a7-b3fe-484f-b5e1-f358ac35bdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f63ead-d2d3-4a0b-842a-b3ebb0e32e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input conditions for extension module my_module have changed. Bumping to version 1 and re-building as my_module_v1...\n",
      "Emitting ninja build file /tmp/build.ninja...\n",
      "Building extension module my_module_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=my_module_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /tmp/main.cpp -o main.o \n",
      "[2/2] c++ main.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o my_module_v1.so\n",
      "Hello World!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module my_module_v1...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cpp_source = \"\"\"\n",
    "std::string hello_world() {\n",
    "  return \"Hello World!\";\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "my_module = load_inline(\n",
    "    name='my_module',\n",
    "    cpp_sources=[cpp_source],\n",
    "    functions=['hello_world'],\n",
    "    verbose=True,\n",
    "    build_directory='/tmp'\n",
    ")\n",
    "\n",
    "print(my_module.hello_world())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d04480b-61f9-4e17-be45-6a55927a54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d07a32-838d-4aef-b1a8-48b24424b73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Ninja\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Installing collected packages: Ninja\n",
      "Successfully installed Ninja-1.11.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebe860-236f-436a-93e0-f0e779ae6cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201f643-3972-41ec-8bea-42ae0c4cd6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f66ccbf-5a6e-4d4d-b555-2b004ce156e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Look at this test for inspiration\n",
    "# https://github.com/pytorch/pytorch/blob/main/test/test_cpp_extensions_jit.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# Define the CUDA kernel and C++ wrapper\n",
    "cuda_source = '''\n",
    "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < height && col < width) {\n",
    "        int idx = row * width + col;\n",
    "        result[idx] = matrix[idx] * matrix[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor square_matrix(torch::Tensor matrix) {\n",
    "    const auto height = matrix.size(0);\n",
    "    const auto width = matrix.size(1);\n",
    "\n",
    "    auto result = torch::empty_like(matrix);\n",
    "\n",
    "    dim3 threads_per_block(16, 16);\n",
    "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
    "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
    "\n",
    "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
    "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
    "\n",
    "    return result;\n",
    "    }\n",
    "'''\n",
    "\n",
    "cpp_source = \"torch::Tensor square_matrix(torch::Tensor matrix);\"\n",
    "\n",
    "# Load the CUDA kernel as a PyTorch extension\n",
    "square_matrix_extension = load_inline(\n",
    "    name='square_matrix_extension',\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=['square_matrix'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='./load_inline_cuda',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")\n",
    "\n",
    "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
    "print(square_matrix_extension.square_matrix(a))\n",
    "\n",
    "# (cudamode) ubuntu@ip-172-31-9-217:~/cudamode/cudamodelecture1$ python load_inline.py \n",
    "# tensor([[ 1.,  4.,  9.],\n",
    "#         [16., 25., 36.]], device='cuda:0')\n",
    "\n",
    "\n",
    "## No great interaction with ncu\n",
    "\n",
    "# (cudamode) ubuntu@ip-172-31-9-217:~/cudamode/cudamodelecture1$ ncu python load_inline.py \n",
    "# ==PROF== Connected to process 55916 (/opt/conda/envs/cudamode/bin/python3.10)\n",
    "# /opt/conda/envs/cudamode/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 36: API call is not supported in the installed CUDA driver (Triggered internally at /opt/conda/conda-bld/pytorch_1702400410390/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
    "#   return torch._C._cuda_getDeviceCount() > 0\n",
    "# No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
    "# Traceback (most recent call last):\n",
    "#   File \"/home/ubuntu/cudamode/cudamodelecture1/load_inline.py\", line 7, in <module>\n",
    "#     a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
    "#   File \"/opt/conda/envs/cudamode/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 298, in _lazy_init\n",
    "#     torch._C._cuda_init()\n",
    "# RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 36: API call is not supported in the installed CUDA driver\n",
    "# ==PROF== Disconnected from process 55916\n",
    "# ==ERROR== The application returned an error code (1).\n",
    "# ==WARNING== No kernels were profiled.\n",
    "# ==WARNING== Profiling kernels launched by child processes requires the --target-processes all option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01496080-8b8c-4d64-ae0d-dcce3b6b9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./load_inline_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea2bdf-9d20-411d-b0f8-e8c27f66646d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
